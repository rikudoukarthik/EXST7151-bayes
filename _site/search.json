[
  {
    "objectID": "lectures/index.html",
    "href": "lectures/index.html",
    "title": "Lecture notes",
    "section": "",
    "text": "2025-08-25\n\n\n\nFrequentist uses only observed patterns to make decision. Least squares and even maximum likelihood estimator are all frequentist. Bayesian is completely different approach, based on priors (experience, expert opinion, or similar study).\nWhen N is low, F estimates are not very reliable. e.g., coin toss 5 times gives 4 heads, but \\(P = 0.8\\) is not very reliable, and it would be good to incorporate our prior information that the coin is fair (\\(P = 0.5\\)). Also, F doesn’t allow us to calculate bias in parameters, e.g., \\(p(P(H) &gt; 0.5)\\)—it considers these parameters as fixed and not as random variables.\nImportantly, prior information is incorporated in a prior distribution, which is a critical component of Bayesian analysis.\n\n\n\n\n\nAim with the course is to show advantages of Bayesian in certain circumstances. In some cases, B results more interpretable than F.\nBritish mathematician and minister Thomas Bayes. Ideas of Bayes’ Theorem posthumously published in 1763 by Pierce. But strong integration in practice only recently, due to technological revolution (strong compute).\nBayesian versus frequentist dichotomy a thing of the past. BeiBei considers herself to be pragmatic, using F approaches for simple problems, large sample sizes, or when no priors, even though 90%+ of her research uses B.\n\\(\\theta\\) is prior, \\(D\\) is data. Posterior distribution = \\(p( \\theta | D )\\), i.e., Bayesian estimate. \\(p(D)\\) is marginal likelihood, very difficult to estimate, but MCMC methods do not require this estimation.\nSingle-parameter models are easy to fit under B (e.g., binomial, poisson). Multiple-parameter models are difficult (e.g., normal distribution, simple regression).\n\n\n\n\n\nUncertainty/confidence:\n\nCorrect interpretation of F CI: if you take a large number of samples and construct CI for each, then 95% of those intervals will overlap the true value. –&gt; not very intuitive or interpretable, not very useful.\nIn B, since parameters are treated as random variables, we CAN estimate an interval such that the parameter is in that interval with exact probability—which is the more intuitive definition of “uncertainty”.\nOne way of calculating CI in F is Wald CI (normal approximation), but poor when \\(N\\) is low. Other better alternatives sometimes approach B style.\n\np-value:\n\nIn F, interpreted as \\(p(\\text{outcome at least as extreme as observed outcome} | H_0)\\), not \\(p(H_0)\\).\nBut B allows us to estimate \\(p(H0)\\) and \\(p(H1)\\)\n\nData samples:\n\nF assumes data are a repeatable random sample\nB considers data as observed from the realised sample—data are fixed, do not care about data not observed.\n\nInference:\n\nIn F, inference is based on likelihood, \\(L(D|\\theta)\\) (data|parameter)\nIn B, inference is based on posterior estimate of parameter, \\(P(\\theta|D)\\) (prior|data)"
  },
  {
    "objectID": "lectures/index.html#quick-why",
    "href": "lectures/index.html#quick-why",
    "title": "Lecture notes",
    "section": "",
    "text": "Frequentist uses only observed patterns to make decision. Least squares and even maximum likelihood estimator are all frequentist. Bayesian is completely different approach, based on priors (experience, expert opinion, or similar study).\nWhen N is low, F estimates are not very reliable. e.g., coin toss 5 times gives 4 heads, but \\(P = 0.8\\) is not very reliable, and it would be good to incorporate our prior information that the coin is fair (\\(P = 0.5\\)). Also, F doesn’t allow us to calculate bias in parameters, e.g., \\(p(P(H) &gt; 0.5)\\)—it considers these parameters as fixed and not as random variables.\nImportantly, prior information is incorporated in a prior distribution, which is a critical component of Bayesian analysis."
  },
  {
    "objectID": "lectures/index.html#what",
    "href": "lectures/index.html#what",
    "title": "Lecture notes",
    "section": "",
    "text": "Aim with the course is to show advantages of Bayesian in certain circumstances. In some cases, B results more interpretable than F.\nBritish mathematician and minister Thomas Bayes. Ideas of Bayes’ Theorem posthumously published in 1763 by Pierce. But strong integration in practice only recently, due to technological revolution (strong compute).\nBayesian versus frequentist dichotomy a thing of the past. BeiBei considers herself to be pragmatic, using F approaches for simple problems, large sample sizes, or when no priors, even though 90%+ of her research uses B.\n\\(\\theta\\) is prior, \\(D\\) is data. Posterior distribution = \\(p( \\theta | D )\\), i.e., Bayesian estimate. \\(p(D)\\) is marginal likelihood, very difficult to estimate, but MCMC methods do not require this estimation.\nSingle-parameter models are easy to fit under B (e.g., binomial, poisson). Multiple-parameter models are difficult (e.g., normal distribution, simple regression)."
  },
  {
    "objectID": "lectures/index.html#interpretability-of-f-b",
    "href": "lectures/index.html#interpretability-of-f-b",
    "title": "Lecture notes",
    "section": "",
    "text": "Uncertainty/confidence:\n\nCorrect interpretation of F CI: if you take a large number of samples and construct CI for each, then 95% of those intervals will overlap the true value. –&gt; not very intuitive or interpretable, not very useful.\nIn B, since parameters are treated as random variables, we CAN estimate an interval such that the parameter is in that interval with exact probability—which is the more intuitive definition of “uncertainty”.\nOne way of calculating CI in F is Wald CI (normal approximation), but poor when \\(N\\) is low. Other better alternatives sometimes approach B style.\n\np-value:\n\nIn F, interpreted as \\(p(\\text{outcome at least as extreme as observed outcome} | H_0)\\), not \\(p(H_0)\\).\nBut B allows us to estimate \\(p(H0)\\) and \\(p(H1)\\)\n\nData samples:\n\nF assumes data are a repeatable random sample\nB considers data as observed from the realised sample—data are fixed, do not care about data not observed.\n\nInference:\n\nIn F, inference is based on likelihood, \\(L(D|\\theta)\\) (data|parameter)\nIn B, inference is based on posterior estimate of parameter, \\(P(\\theta|D)\\) (prior|data)"
  },
  {
    "objectID": "lectures/index.html#prior-and-posterior",
    "href": "lectures/index.html#prior-and-posterior",
    "title": "Lecture notes",
    "section": "Prior and posterior",
    "text": "Prior and posterior\n\nIn many cases parameter of interest (\\(\\theta\\)) is related to proportion/prevalence/probability.\n\nIn these cases, distribution of data (\\(D\\)) is binomial, i.e., probability of something. This is similar to F methods, e.g., using GLMs with binomial distribution to infer about probability/proportion of something.\nBut distribution of parameter (\\(\\theta\\)) is beta, i.e., proportion of something. This distribution is unique to B methods, and so is having to think about the appropriate parameter space distribution.\nBeta distribution has two parameters \\(\\alpha\\) and \\(\\beta\\), and the sample size \\(N = \\alpha + \\beta\\). Again, an intuitive way of describing the distribution of probability/proportion/odds? parameters.\n\\(B = [0, 1]\\), i.e., truncated normal distribution.\n\nEstimate of \\(\\theta\\):\n\nF gives point estimate, although there is the possibility of calculating non-intuitive CIs\nEstimate in B is a probability distribution (i.e., posterior distribution), and it is possible to also calculate summary stats of the distribution, like mean, median, etc.\nThe y-axis of such PDFs can be interpreted as the likelihoods of each probability value on the x-axis—each of which can be thought of a F estimate.\n\nLikelihood is therefore important in both F and B! F uses maximum likelihood estimator (MLE).\n\n\n# explore probability mass function of binomial distribution\n\n# probability of getting 0 successes out of 20 when p(success) is 0.5\ndbinom(0, 20, 0.5) \n## [1] 9.536743e-07\nplot(0:20, dbinom(0:20, 20, 0.5)) \n\n# when success prob is lower:\nplot(0:20, dbinom(0:20, 20, 0.05)) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompromise between prior and data:\n\nIf \\(N(D)\\) is large, posterior estimate tends closer to \\(D\\), but if small tends closer to prior \\(\\theta\\).\nOn the other hand, if confidence in prior is high (i.e., low \\(sd(\\theta)\\)), posterior estimate tends closer to prior \\(\\theta\\).\nMajor critique from F side about B methods: not objective enough, due to contingence on prior definitions (mean and sd). Prior distribution has to be justified! (Contingence on \\(N(D)\\) is shared between F and B.)\nRule of thumb for beta distribution: \\(sd(\\theta)\\) = 0.05 is very confident, \\(sd(\\theta)\\) = 0.2 is very unconfident (means 95% between 0.1 and 0.9 for mean = 0.5)."
  },
  {
    "objectID": "lectures/index.html#defining-priors",
    "href": "lectures/index.html#defining-priors",
    "title": "Lecture notes",
    "section": "Defining priors",
    "text": "Defining priors\n\nThree ways of calculating priors:\n\nConjugate: when both prior and posterior follow the same data distribution family. Generally easier computation and convenient.\nHistorical: use historical info (e.g., fair coin p = 0.5)\nNon-informative prior: no prior information, e.g., uniform distribution, or U-shaped (unfair coin). (Why is latter considered non-informative? Will see later in course.)\n\nDistribution families:\n\nBeta is a truncated normal distribution. We can also use truncated t distribution (t is similar to normal, just with fatter tails), or truncated chi-sq (like normal but only positive values) or truncated F.\nBut some of these are difficult to get summary values from if posterior also that distribution: they don’t give closed-form distributions (area under curve sums to 1).\nBeta priors on the other hand give closed-form posterior distributions and therefore allow easy summaries—for binomial sampling distribution of data.\nWhat prior distribution gives closed-form posterior distribution depends on sampling model distribution (distribution of data). Under binomial sampling distribution, a beta prior leads to a beta posterior. But e.g. normal sampling beta prior does not give closed-form posterior."
  },
  {
    "objectID": "lectures/index.html#posterior-estimate",
    "href": "lectures/index.html#posterior-estimate",
    "title": "Lecture notes",
    "section": "Posterior estimate",
    "text": "Posterior estimate\n\nBayesian estimate gives an updated \\(\\theta\\) (posterior) distribution, which gives us even more info like:\n\nProbabilites that the \\(\\theta\\) lies in a given interval\nBut also, conversely, defines interval (of given confidence, e.g, 95%) in which the parameter lies—the posterior credible interval, which has a probability interpretation.\n\nFor estimating population mean:\n\nF formula is pop-mean = sample-mean\nB posterior mean (which is a summary value) is weighted average of sample mean and prior guess—incorporates sample size, prior confidence, and prior mean (\\(N(D)\\), \\(sd(\\theta)\\), \\(\\bar{\\theta}\\))—but formula depends on distribution of prior \\(\\theta\\).\n\n\n\n# explore beta distribution\n\n# theta [0, 1] with corresponding probs for a beta(2, 20) distribution\nplot(seq(0, 1, 0.01), \n     dbeta(seq(0, 1, 0.01), 2, 20)) \n\n# prob that theta &lt; 0.2 for a beta(2, 20) distribution\npbeta(0.2, 2, 20) \n## [1] 0.9423539\n\n\n\n\n\n\n\n\n\n# suppose, after incorporating prior and data, new beta(2, 40) distribution as \n# Bayesian estimate, but this estimate gives us more info than just distribution:\n\n# updated theta [0, 1] with corresponding probs\nplot(seq(0, 1, 0.01), \n     dbeta(seq(0, 1, 0.01), 2, 40)) \n\n# updated prob that theta &lt; 0.2\npbeta(0.2, 2, 40) \n## [1] 0.9988037\n\n# prob that theta &gt; 0.15 given our data and posterior dist\n1 - pbeta(0.15, 2, 40) ; pbeta(0.15, 2, 40, lower.tail = FALSE)\n## [1] 0.01051611\n## [1] 0.01051611\n\n\n\n\n\n\n\n\n\n# these are probabilities that theta is in some interval\n# conversely, posterior 95% credible interval:\nqbeta(c(0.025, 0.975), 2, 40)\n## [1] 0.005963118 0.128554020\n# which means P(0.006 &lt; theta &lt; 0.13 | data) = 0.95\n\n# add CI to figure\nplot(seq(0, 1, 0.01), \n     dbeta(seq(0, 1, 0.01), 2, 40)) \nrect(xleft = qbeta(c(0.025, 0.975), 2, 40)[1], \n     xright = qbeta(c(0.025, 0.975), 2, 40)[2], \n     ybottom = -1, ytop = 20,\n     col = rgb(0,0.5,0.75, alpha = 0.25), border = NA)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Fall 2025 edition of Dr BeiBei Guo’s course on Bayesian Data Analysis (EXST 7151) at Louisiana State University."
  }
]