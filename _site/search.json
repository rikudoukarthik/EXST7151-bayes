[
  {
    "objectID": "lectures/index.html",
    "href": "lectures/index.html",
    "title": "Lecture notes",
    "section": "",
    "text": "L1: Overview\n2025-08-25\n\nFrequentist uses only observed patterns to make decision. Least squares and even maximum likelihood estimator are all frequentist.\nBayesian is completely different approach, based on priors (experience, expert opinion, or similar study).\nWhen N is low, frequentist estimates are not very reliable. e.g., coin toss 5 times gives 4 heads, but P = 0.8 is not very reliable, and it would be good to incorporate our prior information that the coin is fair (P = 0.5). Also, frequentist doesn’t allow us to calculate bias in parameters, i.e., p( P(H) &gt; 0.5)—it considers these parameters as fixed and not as random variables.\nImportantly, prior information is incorporated in a prior distribution, which is a critical component of Bayesian analysis.\nBayesian vs frequentist dichotomy a thing of the past. BeiBei considers herself to be pragmatic, using frequentist approaches for simple problems, large sample sizes, or when no priors, even though 90%+ of her research uses Bayesian.\nAim with the course is to show advantages of Bayesian in certain circumstances. Sometimes, results more interpretable.\nBritish mathematician and minister Thomas Bayes. Ideas of Bayes’ Theorem posthumously published in 1763 by Pierce.\nTheta is prior, D is data. Posterior distribution = Bayesian estimate, i.e., p( theta | D ). p(D) is marginal likelihood, very difficult to estimate, but MCMC methods do not require this estimation.\nSingle-parameter models are easy to fit under Bayesian approach (binomial, poisson). Multiple-parameter models are difficult (normal distribution, simple regression).\nCorrect interpretation of frequentist CI: if you take a large number of samples and construct CI for each, then 95% of those intervals will overlap the true value. –&gt; not very intuitive or interpretable, not very useful. In Bayesian, since parameters are treated as random variables, we CAN estimate an interval such that the parameter is in that interval with exact probability.\nFrequentist p value can be interpreted as p(outcome at least as extreme as observed outcome | H0). Similarly, Bayesian, allows us to estimate p(H0) and p(H1)\nF assumes data are a repeatable random sample (there is a frequency), while B considers data as observed from the realised sample (data are fixed, do not care about data not observed).\nIn F, inference is based on likelihood L(data|parameter), while in B inference is based on posterior estimate of parameter P(parameter(prior)|data)\n\n\n\nL2: Frequentist & Bayesian estimation\n2025-08-27\n\nPrior distributions are often beta distributed: two parameters alpha and beta (and N = alpha + beta)\nIn F, the parameter estimate is a point estimate (with possibility of calculating CIs), while in B, estimate is a probability distribution (posterior distribution, with possibility of calculating summary stat of distribution like mean)\ny-axis of PDF can be interpreted as likelihood of probability values on x-axis (frequentist). Likelihood is important in both F and B! F uses MLE to estimate likelihood.\nIf N is large, posterior will be closer to data, if small it will be closer to prior. Conversely for “confidence” of prior, which has a major influence on posterior estimate.\nMajor F criticism of B is that it doesn’t seem “objective enough” due to contingence on prior definitions (mean and sd). Prior distribution has to be justified!\nGenerally, sd = 0.05 is very confident, sd = 0.2 is very unconfident (95% between 0.1 and 0.9 for mean = 0.5).\nThree ways of calculating priors:\n\nConjugate: when both prior and posterior follow the same data distribution family.\n\nGenerally easier computation and convenient\n\nHistorical: use historical data (e.g., fair coin p = 0.5)\nNon-informative prior: no prior information, e.g., uniform distribution, or U-shaped (unfair coin). (Why is latter considered non-informative? Will see later in course.)\n\nOne way of calculating CI in F is Wald CI (normal approximation), but poor when N is low. Other better alternatives sometimes approach Bayesian style.\nDistribution families:\n\nBeta is a truncated normal distribution\nCan also use truncated t distribution (t is similar to normal, just with fatter tails)\nOr truncated chi-sq (like normal but only positive values) or truncated F\nBut some of these are difficult to get summary values from if posterior also that distribution (don’t give closed-form distributions). Beta on the other hand allows easy summaries.\nWhat prior distribution gives closed-form posterior distribution depends on sampling model distribution (distribution of data). Under binomial sampling distribution, a beta prior leads to a beta posterior. But e.g. normal sampling beta prior does not give closed-form posterior.\n\nBayesian estimate gives an updated posterior distribution, but that info gives us even more like:\n\nProbabilites that the parameter theta is in a given interval\nBut also, conversely, given (e.g, 95%) interval in which the parameter lies (posterior credible interval, with probability interpretation)\n\nFor estimating population mean:\n\nF formula is pop-mean = sample-mean\nB posterior mean (which is summary value) is weighted average of sample mean and prior guess—incorporates sample size, prior precision (confidence in prior), and prior mean—but formula depends on distribution of prior\n\n\n\n# explore probability mass function of binomial distribution\n\ndbinom(0, 20, 0.5) # probability of getting 0 successes out of 20 when p(success) is 0.5\n\n[1] 9.536743e-07\n\ndbinom(10, 20, 0.5) \n\n[1] 0.1761971\n\nplot(0:20, dbinom(0:20, 20, 0.5)) # probability of getting 0 successes out of 20 when p(success) is 0.5\n\n\n\n\n\n\n\nplot(0:20, dbinom(0:20, 20, 0.05)) # lower success prob\n\n\n\n\n\n\n\n# explore beta distribution\n\npbeta(0.2, 2, 20) # prob that parameter theta &lt; 0.2 for a beta(2, 20) distribution\n\n[1] 0.9423539\n\n# after incorporating prior and data, new beta(2, 40) distribution as Bayesian estimate\n# but the Bayesian estimate gives us more info than just distribution\npbeta(0.2, 2, 40) # updated prob as before\n\n[1] 0.9988037\n\n1 - pbeta(0.15, 2, 40) # prob that parameter theta &gt; 0.15 given our data and posterior dist\n\n[1] 0.01051611\n\npbeta(0.15, 2, 40, lower.tail = FALSE)\n\n[1] 0.01051611\n\n# these are probabilities that theta is in some interval\n# conversely, posterior credible interval:\nqbeta(c(0.025, 0.975), 2, 40)\n\n[1] 0.005963118 0.128554020\n\n# which means P(0.006 &lt; theta &lt; 0.13 | data) = 0.95"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Fall 2025 edition of Dr BeiBei Guo’s course on Bayesian Data Analysis (EXST 7151) at Louisiana State University."
  }
]