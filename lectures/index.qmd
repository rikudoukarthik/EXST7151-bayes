---
title: "Lecture notes"
---

# Overview

**2025-08-25**

## Quick why

- Frequentist uses only observed patterns to make decision. Least squares and even maximum likelihood estimator are all frequentist. Bayesian is completely different approach, based on priors (experience, expert opinion, or similar study). 
- When N is low, F estimates are not very reliable. e.g., coin toss 5 times gives 4 heads, but $P = 0.8$ is not very reliable, and it would be good to incorporate our prior information that the coin is fair ($P = 0.5$). Also, F doesn’t allow us to calculate bias in parameters, e.g., $p(P(H) > 0.5)$—it considers these parameters as fixed and not as random variables.
- Importantly, **prior information is incorporated in a prior distribution**, which is a critical component of Bayesian analysis.

## What

- Aim with the course is to show advantages of Bayesian in certain circumstances. In some cases, B results more interpretable than F. 
- British mathematician and minister Thomas Bayes. Ideas of Bayes’ Theorem posthumously published in 1763 by Pierce. But strong integration in practice only recently, due to technological revolution (strong compute).
- Bayesian *versus* frequentist dichotomy a thing of the past. BeiBei considers herself to be pragmatic, using F approaches for simple problems, large sample sizes, or when no priors, even though 90%+ of her research uses B.
- $\theta$ is prior, $D$ is data. Posterior distribution = $p( \theta | D )$, i.e., Bayesian estimate. $p(D)$ is marginal likelihood, very difficult to estimate, but MCMC methods do not require this estimation.
- Single-parameter models are easy to fit under B (e.g., binomial, poisson). Multiple-parameter models are difficult (e.g., normal distribution, simple regression).

## Interpretability of F & B

- Uncertainty/confidence:
    - Correct interpretation of F CI: if you take a large number of samples and construct CI for each, then **95% of those intervals will overlap the true value**. --> not very intuitive or interpretable, not very useful. 
    - In B, since parameters are treated as random variables, we CAN estimate an interval such that the **parameter is in that interval with exact probability**---which is the more intuitive definition of "uncertainty".
    - One way of calculating CI in F is Wald CI (normal approximation), but poor when $N$ is low. Other better alternatives sometimes approach B style. 
- p-value:
    - In F, interpreted as $p(\text{outcome at least as extreme as observed outcome} | H_0)$, not $p(H_0)$. 
    - But B allows us to estimate $p(H0)$ and $p(H1)$
- Data samples:
    - F assumes data are a repeatable random sample
    - B considers data as observed from the realised sample---data are fixed, do not care about data not observed.
- Inference:
    - In F, inference is based on likelihood, $L(D|\theta)$ (data|parameter)
    - In B, inference is based on posterior estimate of parameter, $P(\theta|D)$ (prior|data)

# Frequentist & Bayesian estimation

**2025-08-27**

## Prior and posterior

- In many cases parameter of interest ($\theta$) is related to proportion/prevalence/probability. 
    - In these cases, distribution of data ($D$) is binomial, i.e., probability of something. This is similar to F methods, e.g., using GLMs with binomial distribution to infer about probability/proportion of something. 
    - But distribution of parameter ($\theta$) is beta, i.e., proportion of something. This distribution is unique to B methods, and so is having to think about the appropriate parameter space distribution.
    - Beta distribution has two parameters $\alpha$ and $\beta$, and the sample size $N = \alpha + \beta$. Again, an intuitive way of describing the distribution of probability/proportion/odds? parameters. 
    - $B = [0, 1]$, i.e., truncated normal distribution.
- Estimate of $\theta$:
    - F gives point estimate, although there is the possibility of calculating non-intuitive CIs
    - Estimate in B is a probability distribution (i.e., posterior distribution), and it is possible to also calculate summary stats of the distribution, like mean, median, etc.
    - The y-axis of such PDFs can be interpreted as the likelihoods of each probability value on the x-axis---each of which can be thought of a F estimate.
- Likelihood is therefore important in both F and B! F uses maximum likelihood estimator (MLE).

```{r}
# explore probability mass function of binomial distribution

# probability of getting 0 successes out of 20 when p(success) is 0.5
dbinom(0, 20, 0.5) 
plot(0:20, dbinom(0:20, 20, 0.5)) 

# when success prob is lower:
plot(0:20, dbinom(0:20, 20, 0.05)) 

```

- Compromise between prior and data:
    - If $N(D)$ is large, posterior estimate tends closer to $D$, but if small tends closer to prior $\theta$.
    - On the other hand, if confidence in prior is high (i.e., low $sd(\theta)$), posterior estimate tends closer to prior $\theta$.
    - **Major critique** from F side about B methods: not objective enough, due to contingence on prior definitions (mean and sd). Prior distribution has to be justified! (Contingence on $N(D)$ is shared between F and B.)
    - Rule of thumb for beta distribution: $sd(\theta)$ = 0.05 is very confident, $sd(\theta)$ = 0.2 is very unconfident (means 95% between 0.1 and 0.9 for mean = 0.5).
    
## Defining priors

- Three ways of calculating priors:
    - Conjugate: when both prior and posterior follow the same data distribution family. Generally *easier computation and convenient*.
    - Historical: use historical info (e.g., fair coin p = 0.5)
    - Non-informative prior: no prior information, e.g., uniform distribution, or U-shaped (unfair coin). (Why is latter considered non-informative? Will see later in course.)
- Distribution families:
    - Beta is a truncated normal distribution. We can also use truncated t distribution (t is similar to normal, just with fatter tails), or truncated chi-sq (like normal but only positive values) or truncated F.
    - But some of these are difficult to get summary values from if posterior also that distribution: they don’t give closed-form distributions (area under curve sums to 1). 
    - Beta priors on the other hand give closed-form posterior distributions and therefore allow easy summaries---for binomial sampling distribution of data.
    - What prior distribution gives closed-form posterior distribution depends on sampling model distribution (distribution of data). Under binomial sampling distribution, a beta prior leads to a beta posterior. But e.g. normal sampling beta prior does not give closed-form posterior.

## Posterior estimate

- Bayesian estimate gives an updated $\theta$ (posterior) distribution, which gives us even more info like:
	- Probabilites that the $\theta$ lies in a given interval
	- But also, conversely, defines interval (of given confidence, e.g, 95%) in which the parameter lies---the posterior credible interval, which has a probability interpretation.
- For estimating population mean:
	- F formula is pop-mean = sample-mean
	- B posterior mean (which is a summary value) is weighted average of sample mean and prior guess---incorporates sample size, prior confidence, and prior mean ($N(D)$, $sd(\theta)$, $\bar{\theta}$)---but formula depends on distribution of prior $\theta$.
	
	
```{r}
# explore beta distribution

# theta [0, 1] with corresponding probs for a beta(2, 20) distribution
plot(seq(0, 1, 0.01), 
     dbeta(seq(0, 1, 0.01), 2, 20)) 

# prob that theta < 0.2 for a beta(2, 20) distribution
pbeta(0.2, 2, 20) 
```

```{r}
# suppose, after incorporating prior and data, new beta(2, 40) distribution as 
# Bayesian estimate, but this estimate gives us more info than just distribution:

# updated theta [0, 1] with corresponding probs
plot(seq(0, 1, 0.01), 
     dbeta(seq(0, 1, 0.01), 2, 40)) 

# updated prob that theta < 0.2
pbeta(0.2, 2, 40) 

# prob that theta > 0.15 given our data and posterior dist
1 - pbeta(0.15, 2, 40) ; pbeta(0.15, 2, 40, lower.tail = FALSE)
```

```{r}
# these are probabilities that theta is in some interval
# conversely, posterior 95% credible interval:
qbeta(c(0.025, 0.975), 2, 40)
# which means P(0.006 < theta < 0.13 | data) = 0.95

# add CI to figure
plot(seq(0, 1, 0.01), 
     dbeta(seq(0, 1, 0.01), 2, 40)) 
rect(xleft = qbeta(c(0.025, 0.975), 2, 40)[1], 
     xright = qbeta(c(0.025, 0.975), 2, 40)[2], 
     ybottom = -1, ytop = 20,
     col = rgb(0,0.5,0.75, alpha = 0.25), border = NA)
```

# Probability concepts and Bayes Rule for events

- Unconditional probability: wrt sample space. Conditional probability: wrt **sample space updated based on posterior distribution** (new info).
- In the case of a fair die, for the first roll, $P(A) = \frac{1}{6}$, but based on what first roll outcome was, $P(A)$ changes according to the posterior distribution. 
- $A$ and $B$ are independent if $P(A|B) = P(A)$ or equivalently $P(A \cap B ) = P(A)P(B)$
- Law of total probability: $P(B) = P(B|A)P(A) + P(B|A^C)P(A^C)$ (extended to any number of $A$ partitions making up $S$)
- Bayes rule for two events: $P(A|B) = \frac{P(B|A)P(A)}{P(B)} = \frac{P(B|A)P(A)}{P(B|A)P(A) + P(B|A^C)P(A^C)}$
    - For multiple events: $P(A_j|B) = \frac{P(B|A_j)P(A_j)}{P(B)} = \frac{P(B|A_j)P(A_j)}{\sum_{i=1}^{n} P(B|A_i)P(A_i)}$
- Joint distribution/two-way distribution: combine distribution of sample spaces of two variables together. Marginal distribution is usually the unconditional probability. 
- Bayesian updation of sample space based on posterior distribution does not change relative probabilities of each outcome but adjusts absolute values so that they add to 1 within this new sample space. The new value is old value divided by marginal probability of updated event.

# Bayes Rule (for events) examples, and probability distributions

- One example of widely accepted and non-controversial use of Bayes Rule is diagnostic tests like HIV tests. Here, sample space partition has four events: true positive, true negative, false positive, false negative. 
- Every random variable has an associated probability distribution. For discrete random variables, this is called probability mass function (pmf), while for continuous random variables it is called probability density function (pdf).
- Every random variable also has a cumulative distribution function (cdf; $F(y)$).
- Unlike pmf, cdf is not restricted to sample space. e.g., in Bernoulli trials, cdf is still defined for $y \lt 0$. $F(y) = P(Y \leq y)$.
- cdf of any discrete random variable is a step function. For Bernoulli, in the step function the size of jump at $y = 0$ equals the failure probability and size of jump at $y = 1$ is the success probability.

```{r}
curve(pbinom(x, 1, 0.3), -2, 2)
curve(pbinom(x, 1, 0.5), -2, 2)
```

- Binomial distribution is for series of Bernoulli trials with same success probability. e.g., number of heads in $n$ number of rolls. Bernoulli is a special case of binomial, where $n = 1$

```{r}
curve(pbinom(x, 8, 0.5), -1, 10)
```

- Every random variable also has an **expected value** or mean, where the mean is a weighted average based on the pmf. 
- We can estimate/approximate this mean by sampling a large number of numbers from the pmf and taking their mean.

```{r}
# Bernoulli
rand_nos <- rbinom(10000, 1, 0.1) # success p = 0.1
mean(rand_nos)

mean(rbinom(10, 1, 0.1)); mean(rbinom(100, 1, 0.1)); mean(rbinom(1000, 1, 0.1)); mean(rbinom(10000, 1, 0.1)); mean(rbinom(100000, 1, 0.1))
```

- Poisson sample space is 0 to infinity.
- Continuous random variables have sample space that's uncountable. Beta distribution sample space is the unit interval $[0,1]$.
- For continuous variable, probability of any single value is zero, so instead of pmf we use pdf. Where we sum individual probabilites in pmf, we integrate across in pdf.
- The difference is that for discrete, y axis is limited to 0 and 1, but for continuous this is not the case. Hence, pdf y-axis does not have probability interpretation like pmf, but relative likelihoods still remain.
- The cdf of continuous variable is: $F(a) = P(Y \let a) = \int_{-\infty}^{a} f(y)dy$
- Uniform distribution is not valid for range including infinity, because then the area under the curve doesn't sum to 1. However, in Bayesian stats, we use such distributions when selecting *improper priors*. 
- Normal distribution is symmetric, so cdf at mean value is 0.5.

```{r}
dnorm(0, 0, 1)
pnorm(0, 0, 1)
```

- Gamma is a continuous version of Poisson, so $(0,\infty)$
    - Exponential and chi-square are special cases of gamma.
    
Beta
    
- Uniform distribution is a special case of beta: $U(0, 1) = B(1, 1)$.
- Beta distribution with $\alpha = \beta$ yields symmetric distribution around 0.5.
- When $\alpha \lt 1$ and $\beta \lt 1$ the distribution is U-shaped. 
- Variance increases as $\alpha$ and $\beta$ decrease. 

WHY doesn't y axis for continuous have probability interpretation?

Is it correct to say beta is continuous extension of Bernoulli?

# Joint & conditional distributions, and likelihood function

**2025-09-10**

- Marginal distributions usually denoted using subscript: $f_x(x)$
- Conditional distribution for discrete RV consists of multiple individual conditional probabilities
- Conditional distribution of y given x: $ f(y|x) = P(Y = y | x = x) = \frac{f(x,y)}{f_x(x)} $
- In most cases, we only use, and need, the unnormalised probability distributions (esp. for posterior) because getting normalised (which involves multiplication by a constant) is computationally demanding (??). But in any case, the unnormalised tells us relative proportions.
- Knowing marginal distributions cannot help construct joint distributions, because different joints can have the same marginals
- In multivariate case, just like in univariate, the pmf will become pdf, but here we need double integrals instead of single integrals. (Two dimensions, so one integral for each of x and y.)

Bayes rule for variables...

- Likelihood closely related to pdfs, but definition highlight key difference between F and B paradigms
- In F, parameters assumed fixed, and data are repeatable random sample (x and y random, theta or other params fixed). In B, parameters assumed random, data fixed (observed from realised sample). Likelihood Function is that function of theta given data which equals the pmf/pdf of data given theta.
- Likelihood Function used after data observed
- It is important in both F and B. Maximum Likelihood Estimator of parameter $\mu$ gives sample mean.